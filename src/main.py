from modules.loader import load_documents
from modules.splitter import splitter_docs
from modules.embedding import get_vectorstore
from modules.rag_chain import build_qa_chain
from langchain_ollama import OllamaLLM
import streamlit as st

def main():
    st.title("Chatbot RAG v·ªõi LLaMA3")

    # Clear button
    if st.button("X√≥a h·ªôi tho·∫°i"):
        st.session_state.messages = []

    # Kh·ªüi t·∫°o messages n·∫øu ch∆∞a c√≥
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Hi·ªÉn th·ªã c√°c tin nh·∫Øn c≈©
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # T·∫£i d·ªØ li·ªáu & m√¥ h√¨nh (n√™n cache ƒë·ªÉ kh√¥ng load l·∫°i nhi·ªÅu l·∫ßn)
    @st.cache_resource(show_spinner="üîÑ ƒêang t·∫£i m√¥ h√¨nh v√† d·ªØ li·ªáu...")
    def load_rag_chain():
        file_path = "raw_data/data.txt"
        model = OllamaLLM(model="llama3")
        docs = load_documents(file_path)
        chunks = splitter_docs(docs)
        vectorstore = get_vectorstore(chunks)
        return build_qa_chain(vectorstore, model)

    qa_chain = load_rag_chain()

    # Nh·∫≠n input ng∆∞·ªùi d√πng
    if prompt := st.chat_input("Nh·∫≠p c√¢u h·ªèi..."):
        st.chat_message("user").markdown(prompt)
        st.session_state.messages.append({"role": "user", "content": prompt})

        # G·ªçi m√¥ h√¨nh RAG ƒë·ªÉ tr·∫£ l·ªùi
        response = qa_chain.invoke(prompt)

        st.chat_message("assistant").markdown(response['result'])
        st.session_state.messages.append({"role": "assistant", "content": response['result']})

if __name__ == "__main__":
    main()
